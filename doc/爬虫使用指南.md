# GitHub 仓库爬虫使用指南

## 📋 目录

1. [快速开始](#快速开始)
2. [环境配置](#环境配置)
3. [基本使用](#基本使用)
4. [数据说明](#数据说明)
5. [常见问题](#常见问题)

---

## 🚀 快速开始

### 1. 安装依赖

```bash
pip install requests pandas openpyxl python-dotenv
```

### 2. 配置环境变量

在项目根目录创建 `.env` 文件，配置以下信息：

```env
# GitHub Token（必需）
GITHUB_TOKEN=your_github_token_here

# MaxKB 配置（可选，用于自动上传到知识库）
MAXKB_URL=http://localhost:8080
MAXKB_USERNAME=admin
MAXKB_PASSWORD=your_password
MAXKB_KNOWLEDGE_ID=your_knowledge_id
```

### 3. 运行爬虫

```bash
cd Datapulse/backend/DataProcessor
python github_text_crawler.py
```

---

## ⚙️ 环境配置

### GitHub Token 获取

1. 登录 GitHub
2. 进入 Settings → Developer settings → Personal access tokens → Tokens (classic)
3. 点击 "Generate new token"
4. 选择需要的权限（至少需要 `public_repo` 权限）
5. 复制生成的 token，保存到 `.env` 文件中

### MaxKB 配置（可选）

如果需要自动上传到 MaxKB 知识库：

1. 确保 MaxKB 服务已启动（默认地址：http://localhost:8080）
2. 登录 MaxKB，创建知识库
3. 获取知识库 ID（在知识库页面的 URL 中）
4. 将配置信息填入 `.env` 文件

---

## 📖 基本使用

### 方式一：命令行运行

```bash
python github_text_crawler.py
```

程序会提示输入：
- 仓库所有者（owner）：例如 `pytorch`
- 仓库名称（repo）：例如 `pytorch`

### 方式二：修改代码直接运行

编辑 `github_text_crawler.py` 文件的 `main()` 函数：

```python
if __name__ == '__main__':
    crawler = GitHubTextCrawler()
    
    # 修改这里的仓库信息
    owner = "pytorch"
    repo = "pytorch"
    
    data = crawler.crawl_repository(owner, repo)
    
    # 保存数据
    crawler.save_to_excel(data, owner, repo)
    json_file = crawler.save_to_json(data, owner, repo)
    
    # 处理数据（自动上传到MaxKB）
    crawler.process_data(json_file, enable_maxkb_upload=True)
```

### 爬取的数据类型

爬虫会自动收集以下数据：

1. **仓库基本信息**
   - 名称、描述、Star数、Fork数等
   - 创建时间、更新时间
   - 编程语言、许可证等

2. **README 文档**
   - 完整的 README.md 内容

3. **Issues**
   - Issue 标题、内容、状态
   - 创建时间、更新时间
   - 标签、评论数等

4. **Pull Requests**
   - PR 标题、内容、状态
   - 创建时间、合并时间
   - 评论数、文件变更数等

5. **提交历史**
   - 提交信息、作者
   - 提交时间、文件变更

6. **贡献者信息**
   - 贡献者列表、贡献统计

7. **发布版本**
   - 版本号、发布时间
   - 发布说明

8. **OpenDigger 指标**（自动获取）
   - 活跃度、影响力
   - Issue/PR 响应时间
   - 贡献者统计等

---

## 📁 数据说明

### 数据存储位置

所有数据统一保存在 `Datapulse/backend/DataProcessor/data/` 目录下：

```
data/
├── owner_repo/                          # 项目文件夹
│   ├── owner_repo_text_data_YYYYMMDD_HHMMSS.json    # 原始JSON数据
│   ├── owner_repo_text_data_YYYYMMDD_HHMMSS.xlsx    # Excel格式数据
│   └── owner_repo_text_data_YYYYMMDD_HHMMSS_processed/  # 处理后的数据
│       ├── timeseries_data.json         # 时序数据（JSON）
│       ├── timeseries_by_year.xlsx      # 按年份汇总的时序数据
│       ├── timeseries_by_quarter.xlsx   # 按季度汇总的时序数据
│       ├── text_data_structured.json    # 结构化文本数据
│       ├── text_data_for_training.txt   # 训练用文本数据
│       ├── text_data_overview.xlsx      # 文本数据概览
│       └── processing_summary.json     # 处理摘要
```

### 数据文件说明

#### 1. 原始数据文件

- **JSON 文件**：包含所有爬取的原始数据，便于程序处理
- **Excel 文件**：包含多个工作表，便于人工查看和分析

#### 2. 处理后的数据文件

- **timeseries_data.json**：时序数据，包含各种指标的时间序列
- **timeseries_by_year.xlsx**：按年份汇总的时序数据
- **timeseries_by_quarter.xlsx**：按季度汇总的时序数据
- **text_data_for_training.txt**：格式化的文本数据，用于训练模型
- **text_data_structured.json**：结构化的文本数据，包含文档类型、内容等
- **text_data_overview.xlsx**：文本数据概览，包含文档统计信息

---

## 🔧 常见问题

### Q1: 爬取失败，提示 Token 无效？

**A:** 检查 `.env` 文件中的 `GITHUB_TOKEN` 是否正确配置，确保 token 有足够的权限。

### Q2: 爬取速度很慢？

**A:** 
- GitHub API 有速率限制，程序会自动处理
- 如果数据量很大，可能需要较长时间
- 可以尝试减少爬取的数据量（修改代码中的爬取逻辑）

### Q3: Excel 文件保存失败？

**A:** 
- 检查磁盘空间是否充足
- 确保有写入权限
- 如果文件很大，可能需要较长时间

### Q4: MaxKB 上传失败？

**A:**
- 确保 MaxKB 服务已启动
- 检查 `.env` 文件中的 MaxKB 配置是否正确
- 确认知识库 ID 是否正确
- 查看控制台输出的错误信息

### Q5: 数据保存在哪里？

**A:** 所有数据统一保存在 `Datapulse/backend/DataProcessor/data/` 目录下，按项目名称（owner_repo）组织。

### Q6: 如何只爬取特定类型的数据？

**A:** 可以修改 `github_text_crawler.py` 中的 `crawl_repository()` 方法，注释掉不需要的数据爬取部分。

---

## 💡 使用技巧

1. **批量爬取多个仓库**
   ```python
   repositories = [
       ("pytorch", "pytorch"),
       ("tensorflow", "tensorflow"),
       ("keras-team", "keras")
   ]
   
   for owner, repo in repositories:
       crawler = GitHubTextCrawler()
       data = crawler.crawl_repository(owner, repo)
       crawler.save_to_excel(data, owner, repo)
   ```

2. **只处理数据不上传**
   ```python
   crawler.process_data(json_file, enable_maxkb_upload=False)
   ```

3. **自定义数据保存路径**
   修改 `save_to_excel()` 和 `save_to_json()` 方法中的路径设置

---

## 📞 获取帮助

如果遇到问题：

1. 查看控制台输出的错误信息
2. 检查 `.env` 文件配置是否正确
3. 确认网络连接正常
4. 查看 GitHub API 状态：https://www.githubstatus.com/

---

## 📝 更新日志

- **v1.0** (2025-12-03)
  - 初始版本
  - 支持爬取 GitHub 仓库数据
  - 支持自动上传到 MaxKB
  - 统一数据存储到 data 目录

---

**祝使用愉快！** 🎉

