"""
GitPulse v4.1 è®­ç»ƒè„šæœ¬ - æ—¶åºåº•åº§å¯¹æ¯”å®éªŒ

ç›®çš„ï¼šè¯æ˜æ–‡æœ¬åœ¨ä¸åŒæ—¶åºæ¶æ„ä¸Šçš„æ™®é€‚æ€§ä»·å€¼

å¯¹æ¯”å®éªŒï¼š
1. Transformer + æ–‡æœ¬ vs çº¯ Transformer
2. GRU + æ–‡æœ¬ vs çº¯ GRU

åˆ¤å®šæ ‡å‡†ï¼š
- text_contribution_pct > 0 å³è¯æ˜æ–‡æœ¬æœ‰æ­£å‘è´¡çŒ®
- ä¸éœ€è¦è¾¾åˆ° PatchTST çš„ 10.67%ï¼Œåªè¦ç¨³å®šæ­£å‘å¢ç›Š

ä½¿ç”¨:
    python train_multimodal_v4_1.py --epochs 100
"""

import os
import sys
import json
import argparse
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from transformers import DistilBertTokenizer
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model.multimodal_ts_v4_1 import (
    MultimodalTransformerV4_1, TransformerTSOnlyV4_1,
    MultimodalGRUV4_1, GRUTSOnlyV4_1,
    MultimodalConditionalGRUV4_1,
    count_parameters
)


class GitHubDataset(Dataset):
    def __init__(self, json_path, tokenizer, max_hist_len=128, max_pred_len=32):
        self.tokenizer = tokenizer
        self.max_hist_len = max_hist_len
        self.max_pred_len = max_pred_len
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.samples = data['samples']
        print(f"Loaded {len(self.samples)} samples")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        hist = np.array(sample['Hist'], dtype=np.float32)
        pred = np.array(sample['Pred'], dtype=np.float32)
        n_vars = hist.shape[1] if len(hist.shape) > 1 else 16
        
        if len(hist) > self.max_hist_len:
            hist = hist[-self.max_hist_len:]
        elif len(hist) < self.max_hist_len:
            pad = np.zeros((self.max_hist_len - len(hist), n_vars), dtype=np.float32)
            hist = np.concatenate([pad, hist], axis=0)
        
        if len(pred) > self.max_pred_len:
            pred = pred[:self.max_pred_len]
        elif len(pred) < self.max_pred_len:
            pad = np.zeros((self.max_pred_len - len(pred), n_vars), dtype=np.float32)
            pred = np.concatenate([pred, pad], axis=0)
        
        text = sample.get('Text', '')
        text_encoded = self.tokenizer(
            text, padding='max_length', truncation=True,
            max_length=256, return_tensors='pt'
        )
        
        return {
            'hist': torch.tensor(hist, dtype=torch.float32),
            'pred': torch.tensor(pred, dtype=torch.float32),
            'input_ids': text_encoded['input_ids'].squeeze(0),
            'attention_mask': text_encoded['attention_mask'].squeeze(0)
        }


def train_multimodal(model, train_loader, val_loader, device, epochs, patience, 
                     model_name, output_dir, lr=5e-4, lambda_cl=0.1, lambda_ml=0.05):
    """è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹"""
    criterion = nn.MSELoss()
    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-5)
    
    best_mse = float('inf')
    best_mae = None
    best_rmse = None
    best_epoch = 0
    patience_counter = 0
    
    for epoch in range(1, epochs + 1):
        model.train()
        total_pred = 0
        total_cl_acc = 0
        total_ml_acc = 0
        total_tw = 0
        n_batches = 0
        
        pbar = tqdm(train_loader, desc=f"[{model_name}] Epoch {epoch}")
        for batch in pbar:
            hist = batch['hist'].to(device)
            targets = batch['pred'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            optimizer.zero_grad()
            pred, cl_loss, ml_loss, metrics = model(hist, input_ids, attention_mask, return_auxiliary=True)
            
            pred_loss = criterion(pred, targets)
            total_loss = pred_loss + lambda_cl * cl_loss + lambda_ml * ml_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_pred += pred_loss.item()
            total_cl_acc += metrics['cl_acc']
            total_ml_acc += metrics['ml_acc']
            total_tw += metrics['text_weight']
            n_batches += 1
            
            pbar.set_postfix({
                'loss': f'{pred_loss.item():.4f}',
                'cl': f'{metrics["cl_acc"]:.1%}',
                'tw': f'{metrics["text_weight"]:.2f}'
            })
        
        # éªŒè¯
        model.eval()
        val_mse = 0
        val_mae = 0
        val_samples = 0
        
        with torch.no_grad():
            for batch in val_loader:
                hist = batch['hist'].to(device)
                targets = batch['pred'].to(device)
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                
                pred = model(hist, input_ids, attention_mask, return_auxiliary=False)
                
                val_mse += nn.MSELoss(reduction='sum')(pred, targets).item()
                val_mae += torch.abs(pred - targets).sum().item()
                val_samples += pred.numel()
        
        val_mse /= val_samples
        val_mae /= val_samples
        val_rmse = np.sqrt(val_mse)
        
        scheduler.step(val_mse)
        
        print(f"[{model_name}] Epoch {epoch}: loss={total_pred/n_batches:.4f}, "
              f"val_mse={val_mse:.4f}, cl={total_cl_acc/n_batches:.1%}, tw={total_tw/n_batches:.2f}")
        
        if val_mse < best_mse:
            best_mse = val_mse
            best_mae = val_mae
            best_rmse = val_rmse
            best_epoch = epoch
            patience_counter = 0
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'val_mse': val_mse
            }, os.path.join(output_dir, f'best_model_{model_name}.pt'))
            print(f"  -> Saved (MSE={val_mse:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
    
    return best_mse, best_mae, best_rmse, best_epoch


def train_ts_only(model, train_loader, val_loader, device, epochs, patience, 
                  model_name, output_dir, lr=1e-3):
    """è®­ç»ƒçº¯æ—¶åºæ¨¡å‹"""
    criterion = nn.MSELoss()
    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-5)
    
    best_mse = float('inf')
    best_mae = None
    best_rmse = None
    best_epoch = 0
    patience_counter = 0
    
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0
        
        pbar = tqdm(train_loader, desc=f"[{model_name}] Epoch {epoch}")
        for batch in pbar:
            hist = batch['hist'].to(device)
            targets = batch['pred'].to(device)
            
            optimizer.zero_grad()
            pred = model(hist)
            loss = criterion(pred, targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # éªŒè¯
        model.eval()
        val_mse = 0
        val_mae = 0
        val_samples = 0
        
        with torch.no_grad():
            for batch in val_loader:
                hist = batch['hist'].to(device)
                targets = batch['pred'].to(device)
                pred = model(hist)
                
                val_mse += nn.MSELoss(reduction='sum')(pred, targets).item()
                val_mae += torch.abs(pred - targets).sum().item()
                val_samples += pred.numel()
        
        val_mse /= val_samples
        val_mae /= val_samples
        val_rmse = np.sqrt(val_mse)
        
        scheduler.step(val_mse)
        
        print(f"[{model_name}] Epoch {epoch}: loss={total_loss/len(train_loader):.4f}, val_mse={val_mse:.4f}")
        
        if val_mse < best_mse:
            best_mse = val_mse
            best_mae = val_mae
            best_rmse = val_rmse
            best_epoch = epoch
            patience_counter = 0
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'val_mse': val_mse
            }, os.path.join(output_dir, f'best_model_{model_name}.pt'))
            print(f"  -> Saved (MSE={val_mse:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
    
    return best_mse, best_mae, best_rmse, best_epoch


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_path', type=str, default='../Pretrain-data/github_multivar.json')
    parser.add_argument('--output_dir', type=str, default='./checkpoints')
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--lr', type=float, default=5e-4)
    parser.add_argument('--hist_len', type=int, default=128)
    parser.add_argument('--pred_len', type=int, default=32)
    parser.add_argument('--d_model', type=int, default=128)
    parser.add_argument('--lambda_cl', type=float, default=0.1)
    parser.add_argument('--lambda_ml', type=float, default=0.05)
    parser.add_argument('--min_text_weight', type=float, default=0.1)
    parser.add_argument('--max_text_weight', type=float, default=0.3)
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("GitPulse v4.1 - æ—¶åºåº•åº§å¯¹æ¯”å®éªŒ")
    print("ç›®çš„ï¼šè¯æ˜æ–‡æœ¬åœ¨ä¸åŒæ¶æ„ä¸Šçš„æ™®é€‚æ€§ä»·å€¼")
    print("=" * 70)
    print(f"Device: {args.device}")
    print(f"Text weight: [{args.min_text_weight}, {args.max_text_weight}]")
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(script_dir, args.data_path) if not os.path.isabs(args.data_path) else args.data_path
    
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    dataset = GitHubDataset(data_path, tokenizer, args.hist_len, args.pred_len)
    
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    print(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}")
    
    patience = 20
    results = {}
    
    # ==================== 1. Transformer + æ–‡æœ¬ ====================
    print("\n" + "=" * 70)
    print("1. Training Transformer + Text")
    print("=" * 70)
    
    transformer_mm = MultimodalTransformerV4_1(
        n_vars=16, hist_len=args.hist_len, pred_len=args.pred_len,
        d_model=args.d_model, min_text_weight=args.min_text_weight, max_text_weight=args.max_text_weight
    ).to(args.device)
    
    print(f"å‚æ•°é‡: {count_parameters(transformer_mm) / 1e6:.3f}M")
    
    mse, mae, rmse, epoch = train_multimodal(
        transformer_mm, train_loader, val_loader, args.device,
        args.epochs, patience, 'transformer_mm', args.output_dir,
        args.lr, args.lambda_cl, args.lambda_ml
    )
    results['Transformer+Text'] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'epoch': epoch}
    
    # ==================== 2. çº¯ Transformer ====================
    print("\n" + "=" * 70)
    print("2. Training Transformer (TS-only)")
    print("=" * 70)
    
    transformer_ts = TransformerTSOnlyV4_1(
        n_vars=16, hist_len=args.hist_len, pred_len=args.pred_len, d_model=args.d_model
    ).to(args.device)
    
    print(f"å‚æ•°é‡: {count_parameters(transformer_ts) / 1e6:.3f}M")
    
    mse, mae, rmse, epoch = train_ts_only(
        transformer_ts, train_loader, val_loader, args.device,
        args.epochs, patience, 'transformer_ts', args.output_dir, 1e-3
    )
    results['Transformer'] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'epoch': epoch}
    
    # ==================== 3. GRU + æ–‡æœ¬ ====================
    print("\n" + "=" * 70)
    print("3. Training GRU + Text")
    print("=" * 70)
    
    gru_mm = MultimodalGRUV4_1(
        n_vars=16, hist_len=args.hist_len, pred_len=args.pred_len,
        d_model=args.d_model, min_text_weight=args.min_text_weight, max_text_weight=args.max_text_weight
    ).to(args.device)
    
    print(f"å‚æ•°é‡: {count_parameters(gru_mm) / 1e6:.3f}M")
    
    mse, mae, rmse, epoch = train_multimodal(
        gru_mm, train_loader, val_loader, args.device,
        args.epochs, patience, 'gru_mm', args.output_dir,
        args.lr, args.lambda_cl, args.lambda_ml
    )
    results['GRU+Text'] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'epoch': epoch}
    
    # ==================== 4. çº¯ GRU ====================
    print("\n" + "=" * 70)
    print("4. Training GRU (TS-only)")
    print("=" * 70)
    
    gru_ts = GRUTSOnlyV4_1(
        n_vars=16, hist_len=args.hist_len, pred_len=args.pred_len, d_model=args.d_model
    ).to(args.device)
    
    print(f"å‚æ•°é‡: {count_parameters(gru_ts) / 1e6:.3f}M")
    
    mse, mae, rmse, epoch = train_ts_only(
        gru_ts, train_loader, val_loader, args.device,
        args.epochs, patience, 'gru_ts', args.output_dir, 1e-3
    )
    results['GRU'] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'epoch': epoch}
    
    # ==================== 5. Conditional GRU + æ–‡æœ¬ï¼ˆv6æœ€ä¼˜ç­–ç•¥ï¼‰ ====================
    print("\n" + "=" * 70)
    print("5. Training Conditional GRU + Text (Best Strategy from v6)")
    print("=" * 70)
    
    cond_gru_mm = MultimodalConditionalGRUV4_1(
        n_vars=16, hist_len=args.hist_len, pred_len=args.pred_len, d_model=args.d_model
    ).to(args.device)
    
    print(f"å‚æ•°é‡: {count_parameters(cond_gru_mm) / 1e6:.3f}M")
    
    mse, mae, rmse, epoch = train_multimodal(
        cond_gru_mm, train_loader, val_loader, args.device,
        args.epochs, patience, 'cond_gru_mm', args.output_dir,
        args.lr, args.lambda_cl, args.lambda_ml
    )
    results['CondGRU+Text'] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'epoch': epoch}
    
    # ==================== ç»“æœæ±‡æ€» ====================
    print("\n" + "=" * 70)
    print("å®éªŒç»“æœæ±‡æ€»")
    print("=" * 70)
    
    print(f"\n{'Model':<25} {'MSE':<10} {'MAE':<10} {'RMSE':<10} {'Epoch'}")
    print("-" * 65)
    
    for name, r in results.items():
        print(f"{name:<25} {r['mse']:<10.4f} {r['mae']:<10.4f} {r['rmse']:<10.4f} {r['epoch']}")
    
    # ==================== æ–‡æœ¬è´¡çŒ®åˆ†æ ====================
    print("\n" + "=" * 70)
    print("æ–‡æœ¬è´¡çŒ®åˆ†æ (text_contribution_pct)")
    print("=" * 70)
    
    # Transformer
    transformer_contrib = (results['Transformer']['mse'] - results['Transformer+Text']['mse']) / results['Transformer']['mse'] * 100
    print(f"\nTransformer:")
    print(f"  çº¯æ—¶åº MSE: {results['Transformer']['mse']:.4f}")
    print(f"  +æ–‡æœ¬ MSE: {results['Transformer+Text']['mse']:.4f}")
    print(f"  æ–‡æœ¬è´¡çŒ®: {transformer_contrib:+.2f}%")
    
    if transformer_contrib > 0:
        print(f"  âœ… æ–‡æœ¬å¯¹ Transformer æœ‰ {transformer_contrib:.2f}% çš„æ­£å‘è´¡çŒ®")
    else:
        print(f"  âš  æ–‡æœ¬å¯¹ Transformer è´¡çŒ®ä¸ºè´Ÿ ({transformer_contrib:.2f}%)")
    
    # GRUï¼ˆæ™®é€šèåˆï¼‰
    gru_contrib = (results['GRU']['mse'] - results['GRU+Text']['mse']) / results['GRU']['mse'] * 100
    print(f"\nGRU (æ™®é€šèåˆ):")
    print(f"  çº¯æ—¶åº MSE: {results['GRU']['mse']:.4f}")
    print(f"  +æ–‡æœ¬ MSE: {results['GRU+Text']['mse']:.4f}")
    print(f"  æ–‡æœ¬è´¡çŒ®: {gru_contrib:+.2f}%")
    
    if gru_contrib > 0:
        print(f"  âœ… æ–‡æœ¬å¯¹ GRU æœ‰ {gru_contrib:.2f}% çš„æ­£å‘è´¡çŒ®")
    else:
        print(f"  âš  æ–‡æœ¬å¯¹ GRU è´¡çŒ®ä¸ºè´Ÿ ({gru_contrib:.2f}%)")
    
    # Conditional GRUï¼ˆv6æœ€ä¼˜ç­–ç•¥ï¼‰
    cond_gru_contrib = (results['GRU']['mse'] - results['CondGRU+Text']['mse']) / results['GRU']['mse'] * 100
    print(f"\nConditional GRU (v6æœ€ä¼˜ç­–ç•¥):")
    print(f"  çº¯æ—¶åº MSE: {results['GRU']['mse']:.4f}")
    print(f"  +æ–‡æœ¬ MSE: {results['CondGRU+Text']['mse']:.4f}")
    print(f"  æ–‡æœ¬è´¡çŒ®: {cond_gru_contrib:+.2f}%")
    
    if cond_gru_contrib > 0:
        print(f"  âœ… æ–‡æœ¬å¯¹ Conditional GRU æœ‰ {cond_gru_contrib:.2f}% çš„æ­£å‘è´¡çŒ®")
    else:
        print(f"  âš  æ–‡æœ¬å¯¹ Conditional GRU è´¡çŒ®ä¸ºè´Ÿ ({cond_gru_contrib:.2f}%)")
    
    # ==================== ç»“è®º ====================
    print("\n" + "=" * 70)
    print("ç»“è®º")
    print("=" * 70)
    
    positive_count = sum([transformer_contrib > 0, gru_contrib > 0, cond_gru_contrib > 0])
    
    if positive_count >= 2:
        print("\nğŸ† æ–‡æœ¬åœ¨å¤šæ•°æ—¶åºæ¶æ„ä¸Šéƒ½æœ‰æ­£å‘è´¡çŒ®ï¼")
        print("   â†’ è¯æ˜äº†æ–‡æœ¬ä¿¡æ¯çš„æ™®é€‚æ€§ä»·å€¼")
    elif positive_count == 1:
        print("\nâœ“ æ–‡æœ¬åœ¨éƒ¨åˆ†æ¶æ„ä¸Šæœ‰æ­£å‘è´¡çŒ®")
    else:
        print("\nâš  å½“å‰å®éªŒä¸­æ–‡æœ¬è´¡çŒ®æœ‰é™")
    
    # å¯¹æ¯” PatchTST (v4) çš„ 10.67%
    print(f"\nğŸ“Š æ–‡æœ¬è´¡çŒ®æ±‡æ€»:")
    print(f"   PatchTST (v4 baseline): +10.67%")
    print(f"   Transformer: {transformer_contrib:+.2f}%")
    print(f"   GRU (æ™®é€šèåˆ): {gru_contrib:+.2f}%")
    print(f"   Conditional GRU (v6æœ€ä¼˜): {cond_gru_contrib:+.2f}%")
    
    # æ‰¾æœ€ä¼˜
    best_contrib = max(transformer_contrib, gru_contrib, cond_gru_contrib)
    if cond_gru_contrib == best_contrib:
        print(f"\nğŸ† Conditional GRU æ˜¯æœ€ä¼˜èåˆç­–ç•¥ ({cond_gru_contrib:+.2f}%)")
    
    # ä¿å­˜ç»“æœ
    final_results = {
        'results': results,
        'text_contribution': {
            'Transformer': transformer_contrib,
            'GRU': gru_contrib,
            'CondGRU': cond_gru_contrib,
            'PatchTST_v4_reference': 10.67
        },
        'conclusion': {
            'transformer_positive': transformer_contrib > 0,
            'gru_positive': gru_contrib > 0,
            'cond_gru_positive': cond_gru_contrib > 0,
            'best_strategy': 'CondGRU' if cond_gru_contrib == best_contrib else ('Transformer' if transformer_contrib == best_contrib else 'GRU')
        }
    }
    
    with open(os.path.join(args.output_dir, 'v4_1_comparison_results.json'), 'w') as f:
        json.dump(final_results, f, indent=2)
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}/v4_1_comparison_results.json")
    print("\n" + "=" * 70)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    main()

